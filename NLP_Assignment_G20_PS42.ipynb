{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# NLP Assignment"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"NLP Assignment",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Group No - 20\n",
    "\n",
    "## Group Member Names:\n",
    "1. Varinder Singh - 2021fc04070@wilp.bits-pilani.ac.in\n",
    "2. Bandaru Raja Sekhar - 2021fc04074@wilp.bits-pilani.ac.in\n",
    "3. Mikhil P.A. - 2021fc04326@wilp.bits-pilani.ac.in\n",
    "4. Yogesh Tiwari - 2021fc04825@wilp.bits-pilani.ac.in"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"iX9yREZtKdMFso0RppTS7G",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Problem Statement - 42\n",
    " \n",
    "## Set 8\n",
    "\n",
    "Amazon Alexa Reviews from URL: https:\/\/www.kaggle.com\/datasets\/sid321axn\/amazon-alexa-reviews\n",
    "\n",
    "This dataset consists of nearly 3000 Amazon customer reviews (input text), star ratings, date of review, variants, and feedback of various amazon Alexa products like Alexa Echo, Echo dots, Alexa Firesticks, etc. for learning how to train Machine for sentiment analysis.\n",
    "\n",
    " "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"LLSEodHBVzQcu7x4w2G0Gd",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Point 1"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"Point 1",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "\n",
    "### Read the Amazon.csv file and set it as a Data frame called Amazon. Describe the Data set head, No. of rows, columns, and attributes. (3 Marks)"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"dZ5Ewt5wD0YndzIWny2wcD",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "!pip install nltk\n",
    "!pip install spacy"
   ],
   "execution_count":1,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Collecting nltk\r\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0\/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.5\/1.5 MB\u001b[0m \u001b[31m57.9 MB\/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5\/1.5 MB\u001b[0m \u001b[31m33.9 MB\/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: click in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk) (7.1.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk) (2022.10.31)\r\n",
      "Requirement already satisfied: tqdm in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk) (4.65.0)\r\n",
      "Requirement already satisfied: joblib in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk) (1.2.0)\r\n",
      "Installing collected packages: nltk\r\n",
      "Successfully installed nltk-3.8.1\r\n",
      "Requirement already satisfied: spacy in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (3.5.0)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (1.23.5)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (2.0.7)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (1.1.1)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (6.3.0)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (3.0.8)\r\n",
      "Requirement already satisfied: pathy>=0.10.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (0.10.1)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (1.0.4)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (3.0.12)\r\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (8.1.9)\r\n",
      "Requirement already satisfied: packaging>=20.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (23.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (1.10.6)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (1.0.9)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (2.0.8)\r\n",
      "Requirement already satisfied: jinja2 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (67.6.0)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (2.4.6)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (3.3.0)\r\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (0.7.0)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (2.26.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from spacy) (4.65.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from jinja2->spacy) (2.1.2)\r\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"UHfwRmqmy6oIpWU5wD4Yj9",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "dataset_tsv = pd.read_csv(\"amazon_alexa.tsv\", sep='\\t')"
   ],
   "execution_count":2,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"d3u1GSacYk2eTTBl2oVqUi",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "dataset = dataset_tsv.copy()"
   ],
   "execution_count":3,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"5BHYc7wwhGuvsg89k19C7b",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "dataset_tsv.head()"
   ],
   "execution_count":4,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>rating<\/th>\n",
       "      <th>date<\/th>\n",
       "      <th>variation<\/th>\n",
       "      <th>verified_reviews<\/th>\n",
       "      <th>feedback<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>5<\/td>\n",
       "      <td>31-Jul-18<\/td>\n",
       "      <td>Charcoal Fabric<\/td>\n",
       "      <td>Love my Echo!<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>5<\/td>\n",
       "      <td>31-Jul-18<\/td>\n",
       "      <td>Charcoal Fabric<\/td>\n",
       "      <td>Loved it!<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>4<\/td>\n",
       "      <td>31-Jul-18<\/td>\n",
       "      <td>Walnut Finish<\/td>\n",
       "      <td>Sometimes while playing a game, you can answer...<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>5<\/td>\n",
       "      <td>31-Jul-18<\/td>\n",
       "      <td>Charcoal Fabric<\/td>\n",
       "      <td>I have had a lot of fun with this thing. My 4 ...<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>5<\/td>\n",
       "      <td>31-Jul-18<\/td>\n",
       "      <td>Charcoal Fabric<\/td>\n",
       "      <td>Music<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"HeWPFNPKU4QBSjRSm8MftO",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### No. of rows, columns, and Column list"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"9LWbm5AOiJbp2KcP5cp20w",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "print('No. of Rows: ', len(dataset))\n",
    "print('No. of Columns: ', len(dataset.columns))\n",
    "print('Column list: ', list(dataset.columns))"
   ],
   "execution_count":5,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "No. of Rows:  3150\n",
      "No. of Columns:  5\n",
      "Column list:  ['rating', 'date', 'variation', 'verified_reviews', 'feedback']\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"fQ6NHfZQ28yzatiaXIE24E",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Dataset Info"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"sy1zApHqa1ZFvqgOuHscnK",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "print(\"Dataset Shape:\", dataset.shape, \"\\n\")\n",
    "dataset.info()"
   ],
   "execution_count":6,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Dataset Shape: (3150, 5) \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3150 entries, 0 to 3149\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   rating            3150 non-null   int64 \n",
      " 1   date              3150 non-null   object\n",
      " 2   variation         3150 non-null   object\n",
      " 3   verified_reviews  3150 non-null   object\n",
      " 4   feedback          3150 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 123.2+ KB\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"HLIcshvdppZ9RfXbtJB7gU",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Point 2"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"Point 2",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Remove punctuations and stop words from the text in the ‘text’ column (2Marks)"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"l9mReMDlWtDmVh0QDrnDbT",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "execution_count":7,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "[nltk_data] Downloading package wordnet to \/home\/datalore\/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\/punkt.zip.\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "True"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"tmquer5uPbLRyMOFh75cS1",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "dataset['verified_reviews'] = dataset['verified_reviews'].apply(preprocess_text)"
   ],
   "execution_count":8,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"4IaLo7CcZ5mZ52VgL1rAKE",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Displaying few rows from dataset \n",
    "\n",
    "dataset.head()"
   ],
   "execution_count":9,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>rating<\/th>\n",
       "      <th>date<\/th>\n",
       "      <th>variation<\/th>\n",
       "      <th>verified_reviews<\/th>\n",
       "      <th>feedback<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>5<\/td>\n",
       "      <td>31-Jul-18<\/td>\n",
       "      <td>Charcoal Fabric<\/td>\n",
       "      <td>love echo<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>5<\/td>\n",
       "      <td>31-Jul-18<\/td>\n",
       "      <td>Charcoal Fabric<\/td>\n",
       "      <td>loved<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>4<\/td>\n",
       "      <td>31-Jul-18<\/td>\n",
       "      <td>Walnut Finish<\/td>\n",
       "      <td>sometimes playing game answer question correct...<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>5<\/td>\n",
       "      <td>31-Jul-18<\/td>\n",
       "      <td>Charcoal Fabric<\/td>\n",
       "      <td>lot fun thing yr old learns dinosaur control l...<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>5<\/td>\n",
       "      <td>31-Jul-18<\/td>\n",
       "      <td>Charcoal Fabric<\/td>\n",
       "      <td>music<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"kNECFlFpVksMa5R8BmAYRl",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Point 3"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"Point 3",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Create two objects X and y. X will be the 'text' column of Dataset and y will be yelp's 'stars' column. create a CountVectorizer object and split the data into training and testing sets. Train a MultinomialNB model and Display the confusion Matrix (5 Marks)"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"XhoZ5x5an0DRlalVHkSB60",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "execution_count":10,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"AVzvlX9BsxAwiWCP6YuWDU",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "X = dataset['verified_reviews']\n",
    "y = dataset['rating']"
   ],
   "execution_count":11,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"n3OU0NRPI0S8FGqgRZahXF",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "#### CountVectorizer \n",
    "CountVectorizer is a tool used in natural language processing to convert a collection of text documents into a matrix of token counts. It is a type of feature extraction technique used in machine learning, where each row of the matrix represents a document and each column represents a specific word in the vocabulary. The value in each cell of the matrix is the count of how many times that word appears in that particular document. This matrix can then be used as input for various machine learning algorithms."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"aNrAMhsIWgUW2caRpFjSdu",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "cv = CountVectorizer()"
   ],
   "execution_count":12,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"8HUF0MRTs1llhJHfimGJd9",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "X = cv.fit_transform(X)"
   ],
   "execution_count":13,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"HeFo65i1sSVFcndVDFq7yx",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "#### Splitting the dataset into Training and Testing dataset"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"yzgO3oDrZuebOFWqtbx94g",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ],
   "execution_count":14,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"qxyMcqXuspwRV0BKNan3b3",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "#### MultinomialNB (Multinomial Naive Bayes) \n",
    "MultinomialNB (Multinomial Naive Bayes) is a type of Naive Bayes algorithm that is commonly used for text classification tasks, such as spam filtering or sentiment analysis. One of the main reasons to use MultinomialNB is its simplicity and efficiency. It is a fast and easy-to-implement algorithm that can handle large datasets with many features (such as the large number of words in a text corpus) and is less prone to overfitting compared to other more complex models."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"c5DbkWaF791VqoYFzVs0a9",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)"
   ],
   "execution_count":15,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "MultinomialNB()"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"3FJqlXEsN1xzFBBysQwR2e",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "y_pred = nb.predict(X_test)"
   ],
   "execution_count":16,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"2lsKy7BzwTjDDpYb7LwJk3",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ],
   "execution_count":17,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "[[ 12   2   1   5  13]\n",
      " [  1   0   0   8  12]\n",
      " [  1   0   6  12  20]\n",
      " [  1   0   0  15  74]\n",
      " [  1   1   0  13 432]]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"3DdKHdcuME0aGBY3UKMlJC",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "This confusion matrix represents the performance of a classifier model that was used to predict. The rows of the matrix represent the actual ratings given by the reviewers, while the columns represent the predicted ratings made by the classifier. The entries in the matrix count the number of reviews that fall into each category.\n",
    "\n",
    "For example, the entry in the first row and first column (12) represents the number of reviews that were actually rated with 1 star and were correctly predicted to have 1 star by the model. The entry in the second row and fourth column (8) represents the number of reviews that were actually rated with 4 stars but were mistakenly predicted to have 2 stars by the model. Overall, the confusion matrix shows that the model performs well for reviews with a 5-star rating (with 432 correct predictions out of 447), but performs poorly for reviews with a 1-star rating (with only 12 correct predictions out of 33)."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"nj5lUUhWQmmxdS5xqu3u8i",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Calculating the accuracy score of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ],
   "execution_count":18,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Accuracy: 0.7380952380952381\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"WB2qw9JpuhatVQgXFj2O1H",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Point 4"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"Point 4",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Display the HMM POS tagging on the first 4 rows of ‘text’ (3 Marks)"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"FSy2HZtim0JCCTnsF3GCN2",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "HMM POS tagging refers to the use of Hidden Markov Models (HMMs) for part-of-speech (POS) tagging in natural language processing. POS tagging involves assigning a grammatical tag to each word in a sentence, indicating its part of speech, such as noun, verb, adjective, etc. HMMs are statistical models that can capture the probability distribution of a sequence of observed events. In the case of POS tagging, the observed events are the words in a sentence, and the hidden events are the POS tags."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"q4qu1IZ9VF2tlw9XqAaj04",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Downloading the packages from nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('tagsets')"
   ],
   "execution_count":19,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Unzipping taggers\/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Unzipping taggers\/universal_tagset.zip.\n",
      "[nltk_data] Downloading package tagsets to \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Unzipping help\/tagsets.zip.\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "True"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"D4cc8STbp80VDVvdkThIed",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#POS tagging of data from tsv files\n",
    "\n",
    "for i in range(4):\n",
    "    text = dataset_tsv['verified_reviews'][i]\n",
    "    print(text.lower())\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "    print(pos_tags)\n",
    "    print()"
   ],
   "execution_count":20,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "love my echo!\n",
      "[('love', 'VERB'), ('my', 'PRON'), ('echo', 'NOUN'), ('!', '.')]\n",
      "\n",
      "loved it!\n",
      "[('loved', 'VERB'), ('it', 'PRON'), ('!', '.')]\n",
      "\n",
      "sometimes while playing a game, you can answer a question correctly but alexa says you got it wrong and answers the same as you.  i like being able to turn lights on and off while away from home.\n",
      "[('sometimes', 'ADV'), ('while', 'ADP'), ('playing', 'VERB'), ('a', 'DET'), ('game', 'NOUN'), (',', '.'), ('you', 'PRON'), ('can', 'VERB'), ('answer', 'VERB'), ('a', 'DET'), ('question', 'NOUN'), ('correctly', 'ADV'), ('but', 'CONJ'), ('alexa', 'VERB'), ('says', 'VERB'), ('you', 'PRON'), ('got', 'VERB'), ('it', 'PRON'), ('wrong', 'ADJ'), ('and', 'CONJ'), ('answers', 'VERB'), ('the', 'DET'), ('same', 'ADJ'), ('as', 'ADP'), ('you', 'PRON'), ('.', '.'), ('i', 'VERB'), ('like', 'ADP'), ('being', 'VERB'), ('able', 'ADJ'), ('to', 'PRT'), ('turn', 'VERB'), ('lights', 'NOUN'), ('on', 'ADP'), ('and', 'CONJ'), ('off', 'ADP'), ('while', 'ADP'), ('away', 'ADV'), ('from', 'ADP'), ('home', 'NOUN'), ('.', '.')]\n",
      "\n",
      "i have had a lot of fun with this thing. my 4 yr old learns about dinosaurs, i control the lights and play games like categories. has nice sound when playing music as well.\n",
      "[('i', 'NOUN'), ('have', 'VERB'), ('had', 'VERB'), ('a', 'DET'), ('lot', 'NOUN'), ('of', 'ADP'), ('fun', 'NOUN'), ('with', 'ADP'), ('this', 'DET'), ('thing', 'NOUN'), ('.', '.'), ('my', 'PRON'), ('4', 'NUM'), ('yr', 'ADV'), ('old', 'ADJ'), ('learns', 'NOUN'), ('about', 'ADP'), ('dinosaurs', 'NOUN'), (',', '.'), ('i', 'NOUN'), ('control', 'VERB'), ('the', 'DET'), ('lights', 'NOUN'), ('and', 'CONJ'), ('play', 'VERB'), ('games', 'NOUN'), ('like', 'ADP'), ('categories', 'NOUN'), ('.', '.'), ('has', 'VERB'), ('nice', 'ADJ'), ('sound', 'NOUN'), ('when', 'ADV'), ('playing', 'VERB'), ('music', 'NOUN'), ('as', 'ADV'), ('well', 'ADV'), ('.', '.')]\n",
      "\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"hMIUVU39ER2U3cBjyhby5M",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#POS tagging of processed data from point 3\n",
    "\n",
    "for i in range(4):\n",
    "    text = dataset['verified_reviews'][i]\n",
    "    print(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "    print(pos_tags)\n",
    "    print()"
   ],
   "execution_count":21,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "love echo\n",
      "[('love', 'NOUN'), ('echo', 'NOUN')]\n",
      "\n",
      "loved\n",
      "[('loved', 'VERB')]\n",
      "\n",
      "sometimes playing game answer question correctly alexa say got wrong answer like able turn light away home\n",
      "[('sometimes', 'ADV'), ('playing', 'VERB'), ('game', 'NOUN'), ('answer', 'ADJ'), ('question', 'NOUN'), ('correctly', 'ADV'), ('alexa', 'ADJ'), ('say', 'VERB'), ('got', 'VERB'), ('wrong', 'ADJ'), ('answer', 'NOUN'), ('like', 'ADP'), ('able', 'ADJ'), ('turn', 'NOUN'), ('light', 'ADJ'), ('away', 'ADV'), ('home', 'NOUN')]\n",
      "\n",
      "lot fun thing yr old learns dinosaur control light play game like category nice sound playing music well\n",
      "[('lot', 'NOUN'), ('fun', 'ADJ'), ('thing', 'NOUN'), ('yr', 'NOUN'), ('old', 'ADJ'), ('learns', 'VERB'), ('dinosaur', 'ADJ'), ('control', 'NOUN'), ('light', 'ADJ'), ('play', 'NOUN'), ('game', 'NOUN'), ('like', 'ADP'), ('category', 'NOUN'), ('nice', 'ADJ'), ('sound', 'NOUN'), ('playing', 'NOUN'), ('music', 'NOUN'), ('well', 'NOUN')]\n",
      "\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"3UYfLMueeselHMkASahqHu",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Point 5"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"Point 5",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Parse the first 4 rows of ‘text’ using Viterbi Parser ( [Use toy_pcfg1 and toy_pcfg2 to get the probabilistic context-free grammars; use the PCFG suitable for each sentence] (2 marks)"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"V6HJporNSV8mRKAXfMlRNe",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from functools import reduce\n",
    "from nltk.grammar import PCFG\n",
    "from nltk.parse import ViterbiParser"
   ],
   "execution_count":22,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"cGo0Wp1t3u6OsdvRR1aNJ4",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "PCFG stands for Probabilistic Context-Free Grammar, which is a formalism used in natural language processing (NLP) to describe the syntax or structure of a language. It is a type of context-free grammar where each production rule has a probability associated with it, which represents the likelihood of that rule being used in a sentence.\n",
    "\n",
    "PCFGs are useful in NLP for several reasons:\n",
    "\n",
    "- **Parsing**: One important application of PCFGs is in parsing, where the goal is to automatically analyze the structure of a sentence and identify the relationships between its words. The probabilities associated with each rule in a PCFG can be used to guide the parsing process, helping to select the most likely parse tree for a given sentence.\n",
    "\n",
    "- **Language modeling**: PCFGs can also be used to model the probability distribution over sentences in a language. This can be used to generate new sentences or to evaluate the likelihood of a given sentence being grammatically correct.\n",
    "\n",
    "- **Machine translation**: PCFGs can be used in machine translation systems to help align the structure of the source and target languages, which can improve the accuracy of the translation.\n"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"23sJ4R8sSAwF2c8dYoEjVD",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "toy_pcfg1 = PCFG.fromstring(\n",
    "\"\"\"\n",
    "    S -> VP NP [0.5] | VP [0.5]\n",
    "    VP -> V [0.5] | V Det [0.5]\n",
    "    V -> 'love' [0.5] | 'loved' [0.5]\n",
    "    NP -> Det N [1]\n",
    "    Det -> 'my' [0.5] | 'it' [0.5]\n",
    "    N -> 'echo' [1]\n",
    "\"\"\")"
   ],
   "execution_count":23,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"XIZWT6dHoYgoPNKX1Pv7QB",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "toy_pcfg2 = PCFG.fromstring(\n",
    "\"\"\"\n",
    "    S -> ADVP NP1 CONJ NP2 CONJ NP3 PNP CONJ NP4 [0.5] | S1 S2 S3 [0.5]\n",
    "    S1 -> PN V V DET N ADP N ADP DET N [1.0]\n",
    "    S2 -> NP5  NP6 CONJ NP7 [1.0]\n",
    "    S3 -> NP8 ADV NP9 [1.0]\n",
    "    NP5 -> PN NUM ADV ADJ V ADP N [1.0]\n",
    "    NP6 -> PN V DET N [1.0]\n",
    "    NP7 -> V N ADP N [1.0]\n",
    "    NP8 -> V ADJ N [1.0]\n",
    "    NP9 -> V N ADV ADV [1.0]\n",
    "    ADVP -> ADV ADP V DET N  [1.0]\n",
    "    NP1 -> PN V V DET N ADV [1.0] \n",
    "    NUM -> '4' [1.0]\n",
    "    NP2 -> N VNP VNP ADJ [1.0] \n",
    "    VNP -> V PN [1.0]\n",
    "    NP3 -> V DET ADJ ADP PN [1.0] \n",
    "    VP -> V V V [1.0]\n",
    "    NP4 -> ADP ADP ADV ADP N [1.0]\n",
    "    PNP -> PN VP PRT V N ADP [1.0]\n",
    "    AD -> ADP ADP [1]\n",
    "    PRT -> 'to' [1.0]\n",
    "    CONJ -> 'and' [0.7] | 'but' [0.3]\n",
    "    N -> 'lot' [0.05] | 'games' [0.1] | 'fun' [0.05] | 'thing' [0.1] | 'dinosaurs' [0.05] | 'music' [0.1] | 'categories' [0.05] | 'sound' [0.05] | 'game' [0.05] | 'question' [0.1] | 'alexa' [0.1] | 'lights' [0.1] | 'home' [0.1]\n",
    "    PN -> 'my' [0.1] | 'you' [0.7] | 'it' [0.1] | 'i' [0.1]\n",
    "    ADJ -> 'old' [0.25] | 'nice' [0.25] | 'wrong' [0.25] | 'same' [0.25]\n",
    "    ADV -> 'well' [0.1] | 'when' [0.2] | 'yr' [0.1] | 'sometimes' [0.2] | 'correctly' [0.2] | 'away' [0.2]\n",
    "    ADP -> 'of' [0.1] | 'with' [0.1] | 'about' [0.1] | 'while' [0.3] | 'as' [0.1] | 'on' [0.1] | 'from' [0.1] | 'off' [0.1]\n",
    "    V -> 'control' [0.025] | 'have' [0.05] | 'had' [0.05] | 'learns' [0.05] | 'play' [0.125] | 'has' [0.05] | 'playing'  [0.1]  | 'can'  [0.05] | 'answer'  [0.1]  | 'says' [0.1] | 'got' [0.05] | 'answers' [0.05] | 'like' [0.05] | 'being' [0.05] | 'able' [0.05] | 'turn' [0.05]  \n",
    "    DET -> 'a' [0.6] | 'the' [0.3] | 'this' [0.1] | \n",
    "\"\"\")"
   ],
   "execution_count":24,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"eZoKDF9roh2DNsmlbxPJCy",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Getting first four sentences from the dataset"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"bxfCJNBAzjk2y4QmquvJQY",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import string\n",
    "\n",
    "# Code to remove punctuations from sentences\n",
    "def remove_punc(sentence):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    sentence_without_punct = sentence.translate(translator)\n",
    "    return sentence_without_punct.lower()\n",
    "\n",
    "sentences = [\n",
    "        (remove_punc(dataset_tsv[\"verified_reviews\"][0]), toy_pcfg1),\n",
    "        (remove_punc(dataset_tsv[\"verified_reviews\"][1]), toy_pcfg1),\n",
    "        (remove_punc(dataset_tsv[\"verified_reviews\"][2]), toy_pcfg2),\n",
    "        (remove_punc(dataset_tsv[\"verified_reviews\"][3]), toy_pcfg2),\n",
    "    ]\n",
    "sentences"
   ],
   "execution_count":25,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "[('love my echo', <Grammar with 10 productions>),\n",
       " ('loved it', <Grammar with 10 productions>),\n",
       " ('sometimes while playing a game you can answer a question correctly but alexa says you got it wrong and answers the same as you  i like being able to turn lights on and off while away from home',\n",
       "  <Grammar with 78 productions>),\n",
       " ('i have had a lot of fun with this thing my 4 yr old learns about dinosaurs i control the lights and play games like categories has nice sound when playing music as well',\n",
       "  <Grammar with 78 productions>)]"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"W7aBpWgFechuUPtOdfsEe5",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Function to print some summary statistics\n",
    "\n",
    "def print_stats(all_parses, num_parses):\n",
    "    print()\n",
    "    print(\"Time (secs)   # Parses   Average P(parse)\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"%11.4f%11d%19.14f\" % (time, num_parses, average))\n",
    "    parses = all_parses.keys()\n",
    "    if parses:\n",
    "        p = reduce(lambda a, b: a + b.prob(), parses, 0) \/ len(parses)\n",
    "    else:\n",
    "        p = 0\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"%11s%11d%19.14f\" % (\"n\/a\", len(parses), p))\n",
    "\n",
    "    if num_parses > 0 :\n",
    "        print(\"\\n\\n------------------------------------\")\n",
    "        print(\"Printing parses\")\n",
    "        print(\"------------------------------------\")\n",
    "        for parse in parses:\n",
    "            print(parse)"
   ],
   "execution_count":26,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"WLNq4Hl51YHIRrKKEbD4GC",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Taking Row 1 from dataset"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"25NCqPXNp34grraHSZZ98k",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "index = 0\n",
    "sent, grammar = sentences[index]\n",
    "sent"
   ],
   "execution_count":27,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "'love my echo'"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"oPaaOvYxVAE0sg5L0A3SFF",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "**Viterbi parser** is a type of algorithm used in natural language processing (NLP) to analyze and parse sentences by assigning a probability to each possible parse tree of the input sentence, and then selecting the parse tree with the highest probability as the correct one.\n",
    "\n",
    "The Viterbi algorithm is a dynamic programming algorithm that recursively calculates the probability of each possible parse tree of a sentence, given a probabilistic context-free grammar (PCFG) and a set of observed words. It is based on the assumption that the probability of a parse tree can be calculated as the product of the probabilities of its constituent parts (i.e., the probability of a subtree is the product of the probabilities of its children).\n",
    "\n",
    "Viterbi parser is used in various NLP tasks such as part-of-speech tagging, named entity recognition, and syntactic parsing. It is particularly useful in cases where the input sentence is ambiguous and can have multiple possible parse trees. By assigning probabilities to each possible parse tree, Viterbi parser can select the most likely interpretation of the sentence, which is essential for many NLP applications such as machine translation, speech recognition, and information retrieval."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"WjdnpTAdkwVzl5tPRHXyi2",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Tokenize the sentence.\n",
    "tokens = sent.split()\n",
    "\n",
    "parser = ViterbiParser(grammar)\n",
    "all_parses = {}\n",
    "\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(f\"\\nsentence: {sent}\\nparser: {parser}\\ngrammar: {grammar}\")\n",
    "print(\"---------------------------------------------------------\\n\\n\\n\")"
   ],
   "execution_count":28,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "---------------------------------------------------------\n",
      "\n",
      "sentence: love my echo\n",
      "parser: <ViterbiParser for <Grammar with 10 productions>>\n",
      "grammar: Grammar with 10 productions (start state = S)\n",
      "    S -> VP NP [0.5]\n",
      "    S -> VP [0.5]\n",
      "    VP -> V [0.5]\n",
      "    VP -> V Det [0.5]\n",
      "    V -> 'love' [0.5]\n",
      "    V -> 'loved' [0.5]\n",
      "    NP -> Det N [1.0]\n",
      "    Det -> 'my' [0.5]\n",
      "    Det -> 'it' [0.5]\n",
      "    N -> 'echo' [1.0]\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"QMKsasu5NhYzFpCQOZ2K80",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import time\n",
    "\n",
    "parser.trace(3)\n",
    "t = time.time()\n",
    "parses = parser.parse_all(tokens)\n",
    "time = time.time() - t\n",
    "average = (\n",
    "    reduce(lambda a, b: a + b.prob(), parses, 0) \/ len(parses) if parses else 0\n",
    ")"
   ],
   "execution_count":29,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Inserting tokens into the most likely constituents table...\n",
      "   Insert: |=..| love\n",
      "   Insert: |.=.| my\n",
      "   Insert: |..=| echo\n",
      "Finding the most likely constituents spanning 1 text elements...\n",
      "   Insert: |=..| V -> 'love' [0.5]                  0.5000000000 \n",
      "   Insert: |=..| VP -> V [0.5]                      0.2500000000 \n",
      "   Insert: |=..| S -> VP [0.5]                      0.1250000000 \n",
      "   Insert: |.=.| Det -> 'my' [0.5]                  0.5000000000 \n",
      "   Insert: |..=| N -> 'echo' [1.0]                  1.0000000000 \n",
      "Finding the most likely constituents spanning 2 text elements...\n",
      "   Insert: |==.| VP -> V Det [0.5]                  0.1250000000 \n",
      "   Insert: |==.| S -> VP [0.5]                      0.0625000000 \n",
      "   Insert: |.==| NP -> Det N [1.0]                  0.5000000000 \n",
      "Finding the most likely constituents spanning 3 text elements...\n",
      "   Insert: |===| S -> VP NP [0.5]                   0.0625000000 \n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"b1n2esZbwSNUrHxlDmhZVS",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "num_parses = len(parses)\n",
    "print(\"Number of parses: \", num_parses)\n",
    "\n",
    "for p in parses:\n",
    "    all_parses[p.freeze()] = 1\n",
    "\n",
    "print(\"---------------------------------------------------------\")\n",
    "print_stats(all_parses, num_parses)\n",
    "index = index + 1"
   ],
   "execution_count":30,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Number of parses:  1\n",
      "---------------------------------------------------------\n",
      "\n",
      "Time (secs)   # Parses   Average P(parse)\n",
      "-----------------------------------------\n",
      "     0.0198          1   0.06250000000000\n",
      "------------------------------------------\n",
      "        n\/a          1   0.06250000000000\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Printing parses\n",
      "------------------------------------\n",
      "(S (VP (V love)) (NP (Det my) (N echo))) [0.0625]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"b64wpfaK64knFdTtNStt0z",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Taking Row 2 from dataset"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"G8cC47FXo43gcD1axVawfT",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "sent, grammar = sentences[index]\n",
    "sent"
   ],
   "execution_count":31,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "'loved it'"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"rxSjfWiP4oTkpcchKkrsRV",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "# Tokenize the sentence.\n",
    "tokens = sent.split()\n",
    "\n",
    "parser = ViterbiParser(grammar)\n",
    "all_parses = {}\n",
    "\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(f\"\\nsentence: {sent}\\nparser: {parser}\\ngrammar: {grammar}\")\n",
    "print(\"---------------------------------------------------------\\n\\n\\n\")"
   ],
   "execution_count":32,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "---------------------------------------------------------\n",
      "\n",
      "sentence: loved it\n",
      "parser: <ViterbiParser for <Grammar with 10 productions>>\n",
      "grammar: Grammar with 10 productions (start state = S)\n",
      "    S -> VP NP [0.5]\n",
      "    S -> VP [0.5]\n",
      "    VP -> V [0.5]\n",
      "    VP -> V Det [0.5]\n",
      "    V -> 'love' [0.5]\n",
      "    V -> 'loved' [0.5]\n",
      "    NP -> Det N [1.0]\n",
      "    Det -> 'my' [0.5]\n",
      "    Det -> 'it' [0.5]\n",
      "    N -> 'echo' [1.0]\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"SlZzflepBnuTyHSSJvGrGq",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import time\n",
    "\n",
    "parser.trace(3)\n",
    "t = time.time()\n",
    "parses = parser.parse_all(tokens)\n",
    "time = time.time() - t\n",
    "average = (\n",
    "    reduce(lambda a, b: a + b.prob(), parses, 0) \/ len(parses) if parses else 0\n",
    ")"
   ],
   "execution_count":33,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Inserting tokens into the most likely constituents table...\n",
      "   Insert: |=.| loved\n",
      "   Insert: |.=| it\n",
      "Finding the most likely constituents spanning 1 text elements...\n",
      "   Insert: |=.| V -> 'loved' [0.5]                  0.5000000000 \n",
      "   Insert: |=.| VP -> V [0.5]                       0.2500000000 \n",
      "   Insert: |=.| S -> VP [0.5]                       0.1250000000 \n",
      "   Insert: |.=| Det -> 'it' [0.5]                   0.5000000000 \n",
      "Finding the most likely constituents spanning 2 text elements...\n",
      "   Insert: |==| VP -> V Det [0.5]                   0.1250000000 \n",
      "   Insert: |==| S -> VP [0.5]                       0.0625000000 \n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"I9nFMlRdMi4pd9hg9bMsTN",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "num_parses = len(parses)\n",
    "print(\"Number of parses: \", num_parses)\n",
    "\n",
    "for p in parses:\n",
    "    all_parses[p.freeze()] = 1\n",
    "\n",
    "print(\"---------------------------------------------------------\")\n",
    "print_stats(all_parses, num_parses)\n",
    "index = index + 1"
   ],
   "execution_count":34,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Number of parses:  1\n",
      "---------------------------------------------------------\n",
      "\n",
      "Time (secs)   # Parses   Average P(parse)\n",
      "-----------------------------------------\n",
      "     0.0367          1   0.06250000000000\n",
      "------------------------------------------\n",
      "        n\/a          1   0.06250000000000\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Printing parses\n",
      "------------------------------------\n",
      "(S (VP (V loved) (Det it))) [0.0625]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"O3ojqNejsJbuNgO0ltIiNh",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Taking Row 3 from dataset"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"7pZ4uCbnhxAiFzJEbM3UgQ",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "sent, grammar = sentences[index]\n",
    "sent"
   ],
   "execution_count":35,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "'sometimes while playing a game you can answer a question correctly but alexa says you got it wrong and answers the same as you  i like being able to turn lights on and off while away from home'"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"EzfVOhwuUSDCjnrhbKcWKL",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "# Tokenize the sentence.\n",
    "tokens = sent.split()\n",
    "\n",
    "parser = ViterbiParser(grammar)\n",
    "all_parses = {}\n",
    "\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(f\"\\nsentence: {sent}\\nparser: {parser}\\ngrammar: {grammar}\")\n",
    "print(\"---------------------------------------------------------\\n\\n\\n\")"
   ],
   "execution_count":36,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "---------------------------------------------------------\n",
      "\n",
      "sentence: sometimes while playing a game you can answer a question correctly but alexa says you got it wrong and answers the same as you  i like being able to turn lights on and off while away from home\n",
      "parser: <ViterbiParser for <Grammar with 78 productions>>\n",
      "grammar: Grammar with 78 productions (start state = S)\n",
      "    S -> ADVP NP1 CONJ NP2 CONJ NP3 PNP CONJ NP4 [0.5]\n",
      "    S -> S1 S2 S3 [0.5]\n",
      "    S1 -> PN V V DET N ADP N ADP DET N [1.0]\n",
      "    S2 -> NP5 NP6 CONJ NP7 [1.0]\n",
      "    S3 -> NP8 ADV NP9 [1.0]\n",
      "    NP5 -> PN NUM ADV ADJ V ADP N [1.0]\n",
      "    NP6 -> PN V DET N [1.0]\n",
      "    NP7 -> V N ADP N [1.0]\n",
      "    NP8 -> V ADJ N [1.0]\n",
      "    NP9 -> V N ADV ADV [1.0]\n",
      "    ADVP -> ADV ADP V DET N [1.0]\n",
      "    NP1 -> PN V V DET N ADV [1.0]\n",
      "    NUM -> '4' [1.0]\n",
      "    NP2 -> N VNP VNP ADJ [1.0]\n",
      "    VNP -> V PN [1.0]\n",
      "    NP3 -> V DET ADJ ADP PN [1.0]\n",
      "    VP -> V V V [1.0]\n",
      "    NP4 -> ADP ADP ADV ADP N [1.0]\n",
      "    PNP -> PN VP PRT V N ADP [1.0]\n",
      "    AD -> ADP ADP [1.0]\n",
      "    PRT -> 'to' [1.0]\n",
      "    CONJ -> 'and' [0.7]\n",
      "    CONJ -> 'but' [0.3]\n",
      "    N -> 'lot' [0.05]\n",
      "    N -> 'games' [0.1]\n",
      "    N -> 'fun' [0.05]\n",
      "    N -> 'thing' [0.1]\n",
      "    N -> 'dinosaurs' [0.05]\n",
      "    N -> 'music' [0.1]\n",
      "    N -> 'categories' [0.05]\n",
      "    N -> 'sound' [0.05]\n",
      "    N -> 'game' [0.05]\n",
      "    N -> 'question' [0.1]\n",
      "    N -> 'alexa' [0.1]\n",
      "    N -> 'lights' [0.1]\n",
      "    N -> 'home' [0.1]\n",
      "    PN -> 'my' [0.1]\n",
      "    PN -> 'you' [0.7]\n",
      "    PN -> 'it' [0.1]\n",
      "    PN -> 'i' [0.1]\n",
      "    ADJ -> 'old' [0.25]\n",
      "    ADJ -> 'nice' [0.25]\n",
      "    ADJ -> 'wrong' [0.25]\n",
      "    ADJ -> 'same' [0.25]\n",
      "    ADV -> 'well' [0.1]\n",
      "    ADV -> 'when' [0.2]\n",
      "    ADV -> 'yr' [0.1]\n",
      "    ADV -> 'sometimes' [0.2]\n",
      "    ADV -> 'correctly' [0.2]\n",
      "    ADV -> 'away' [0.2]\n",
      "    ADP -> 'of' [0.1]\n",
      "    ADP -> 'with' [0.1]\n",
      "    ADP -> 'about' [0.1]\n",
      "    ADP -> 'while' [0.3]\n",
      "    ADP -> 'as' [0.1]\n",
      "    ADP -> 'on' [0.1]\n",
      "    ADP -> 'from' [0.1]\n",
      "    ADP -> 'off' [0.1]\n",
      "    V -> 'control' [0.025]\n",
      "    V -> 'have' [0.05]\n",
      "    V -> 'had' [0.05]\n",
      "    V -> 'learns' [0.05]\n",
      "    V -> 'play' [0.125]\n",
      "    V -> 'has' [0.05]\n",
      "    V -> 'playing' [0.1]\n",
      "    V -> 'can' [0.05]\n",
      "    V -> 'answer' [0.1]\n",
      "    V -> 'says' [0.1]\n",
      "    V -> 'got' [0.05]\n",
      "    V -> 'answers' [0.05]\n",
      "    V -> 'like' [0.05]\n",
      "    V -> 'being' [0.05]\n",
      "    V -> 'able' [0.05]\n",
      "    V -> 'turn' [0.05]\n",
      "    DET -> 'a' [0.6]\n",
      "    DET -> 'the' [0.3]\n",
      "    DET -> 'this' [0.1]\n",
      "    DET ->  [0]\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"Erkxyf1Bnp13todMYV86nh",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import time\n",
    "\n",
    "parser.trace(3)\n",
    "t = time.time()\n",
    "parses = parser.parse_all(tokens)\n",
    "time = time.time() - t\n",
    "average = (\n",
    "    reduce(lambda a, b: a + b.prob(), parses, 0) \/ len(parses) if parses else 0\n",
    ")"
   ],
   "execution_count":37,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Inserting tokens into the most likely constituents table...\n",
      "   Insert: |=.....................................| sometimes\n",
      "   Insert: |.=....................................| while\n",
      "   Insert: |..=...................................| playing\n",
      "   Insert: |...=..................................| a\n",
      "   Insert: |....=.................................| game\n",
      "   Insert: |.....=................................| you\n",
      "   Insert: |......=...............................| can\n",
      "   Insert: |.......=..............................| answer\n",
      "   Insert: |........=.............................| a\n",
      "   Insert: |.........=............................| question\n",
      "   Insert: |..........=...........................| correctly\n",
      "   Insert: |...........=..........................| but\n",
      "   Insert: |............=.........................| alexa\n",
      "   Insert: |.............=........................| says\n",
      "   Insert: |..............=.......................| you\n",
      "   Insert: |...............=......................| got\n",
      "   Insert: |................=.....................| it\n",
      "   Insert: |.................=....................| wrong\n",
      "   Insert: |..................=...................| and\n",
      "   Insert: |...................=..................| answers\n",
      "   Insert: |....................=.................| the\n",
      "   Insert: |.....................=................| same\n",
      "   Insert: |......................=...............| as\n",
      "   Insert: |.......................=..............| you\n",
      "   Insert: |........................=.............| i\n",
      "   Insert: |.........................=............| like\n",
      "   Insert: |..........................=...........| being\n",
      "   Insert: |...........................=..........| able\n",
      "   Insert: |............................=.........| to\n",
      "   Insert: |.............................=........| turn\n",
      "   Insert: |..............................=.......| lights\n",
      "   Insert: |...............................=......| on\n",
      "   Insert: |................................=.....| and\n",
      "   Insert: |.................................=....| off\n",
      "   Insert: |..................................=...| while\n",
      "   Insert: |...................................=..| away\n",
      "   Insert: |....................................=.| from\n",
      "   Insert: |.....................................=| home\n",
      "Finding the most likely constituents spanning 1 text elements...\n",
      "   Insert: |=.....................................| ADV -> 'sometimes' [0.2] 0.2000000000 \n",
      "   Insert: |.=....................................| ADP -> 'while' [0.3] 0.3000000000 \n",
      "   Insert: |..=...................................| V -> 'playing' [0.1] 0.1000000000 \n",
      "   Insert: |...=..................................| DET -> 'a' [0.6] 0.6000000000 \n",
      "   Insert: |....=.................................| N -> 'game' [0.05] 0.0500000000 \n",
      "   Insert: |.....=................................| PN -> 'you' [0.7] 0.7000000000 \n",
      "   Insert: |......=...............................| V -> 'can' [0.05] 0.0500000000 \n",
      "   Insert: |.......=..............................| V -> 'answer' [0.1] 0.1000000000 \n",
      "   Insert: |........=.............................| DET -> 'a' [0.6] 0.6000000000 \n",
      "   Insert: |.........=............................| N -> 'question' [0.1] 0.1000000000 \n",
      "   Insert: |..........=...........................| ADV -> 'correctly' [0.2] 0.2000000000 \n",
      "   Insert: |...........=..........................| CONJ -> 'but' [0.3] 0.3000000000 \n",
      "   Insert: |............=.........................| N -> 'alexa' [0.1] 0.1000000000 \n",
      "   Insert: |.............=........................| V -> 'says' [0.1] 0.1000000000 \n",
      "   Insert: |..............=.......................| PN -> 'you' [0.7] 0.7000000000 \n",
      "   Insert: |...............=......................| V -> 'got' [0.05] 0.0500000000 \n",
      "   Insert: |................=.....................| PN -> 'it' [0.1] 0.1000000000 \n",
      "   Insert: |.................=....................| ADJ -> 'wrong' [0.25] 0.2500000000 \n",
      "   Insert: |..................=...................| CONJ -> 'and' [0.7] 0.7000000000 \n",
      "   Insert: |...................=..................| V -> 'answers' [0.05] 0.0500000000 \n",
      "   Insert: |....................=.................| DET -> 'the' [0.3] 0.3000000000 \n",
      "   Insert: |.....................=................| ADJ -> 'same' [0.25] 0.2500000000 \n",
      "   Insert: |......................=...............| ADP -> 'as' [0.1] 0.1000000000 \n",
      "   Insert: |.......................=..............| PN -> 'you' [0.7] 0.7000000000 \n",
      "   Insert: |........................=.............| PN -> 'i' [0.1] 0.1000000000 \n",
      "   Insert: |.........................=............| V -> 'like' [0.05] 0.0500000000 \n",
      "   Insert: |..........................=...........| V -> 'being' [0.05] 0.0500000000 \n",
      "   Insert: |...........................=..........| V -> 'able' [0.05] 0.0500000000 \n",
      "   Insert: |............................=.........| PRT -> 'to' [1.0] 1.0000000000 \n",
      "   Insert: |.............................=........| V -> 'turn' [0.05] 0.0500000000 \n",
      "   Insert: |..............................=.......| N -> 'lights' [0.1] 0.1000000000 \n",
      "   Insert: |...............................=......| ADP -> 'on' [0.1] 0.1000000000 \n",
      "   Insert: |................................=.....| CONJ -> 'and' [0.7] 0.7000000000 \n",
      "   Insert: |.................................=....| ADP -> 'off' [0.1] 0.1000000000 \n",
      "   Insert: |..................................=...| ADP -> 'while' [0.3] 0.3000000000 \n",
      "   Insert: |...................................=..| ADV -> 'away' [0.2] 0.2000000000 \n",
      "   Insert: |....................................=.| ADP -> 'from' [0.1] 0.1000000000 \n",
      "   Insert: |.....................................=| N -> 'home' [0.1] 0.1000000000 \n",
      "Finding the most likely constituents spanning 2 text elements...\n",
      "   Insert: |.............==.......................| VNP -> V PN [1.0] 0.0700000000 \n",
      "   Insert: |...............==.....................| VNP -> V PN [1.0] 0.0050000000 \n",
      "   Insert: |.................................==...| AD -> ADP ADP [1.0] 0.0300000000 \n",
      "Finding the most likely constituents spanning 3 text elements...\n",
      "   Insert: |.........................===..........| VP -> V V V [1.0] 0.0001250000 \n",
      "Finding the most likely constituents spanning 4 text elements...\n",
      "Finding the most likely constituents spanning 5 text elements...\n",
      "   Insert: |=====.................................| ADVP -> ADV ADP V DET N [1.0] 0.0001800000 \n",
      "   Insert: |...................=====..............| NP3 -> V DET ADJ ADP PN [1.0] 0.0002625000 \n",
      "   Insert: |.................................=====| NP4 -> ADP ADP ADV ADP N [1.0] 0.0000600000 \n",
      "Finding the most likely constituents spanning 6 text elements...\n",
      "   Insert: |.....======...........................| NP1 -> PN V V DET N ADV [1.0] 0.0000420000 \n",
      "   Insert: |............======....................| NP2 -> N VNP VNP ADJ [1.0] 0.0000087500 \n",
      "Finding the most likely constituents spanning 7 text elements...\n",
      "Finding the most likely constituents spanning 8 text elements...\n",
      "   Insert: |........................========......| PNP -> PN VP PRT V N ADP [1.0] 0.0000000063 \n",
      "Finding the most likely constituents spanning 9 text elements...\n",
      "Finding the most likely constituents spanning 10 text elements...\n",
      "Finding the most likely constituents spanning 11 text elements...\n",
      "Finding the most likely constituents spanning 12 text elements...\n",
      "Finding the most likely constituents spanning 13 text elements...\n",
      "Finding the most likely constituents spanning 14 text elements...\n",
      "Finding the most likely constituents spanning 15 text elements...\n",
      "Finding the most likely constituents spanning 16 text elements...\n",
      "Finding the most likely constituents spanning 17 text elements...\n",
      "Finding the most likely constituents spanning 18 text elements...\n",
      "Finding the most likely constituents spanning 19 text elements...\n",
      "Finding the most likely constituents spanning 20 text elements...\n",
      "Finding the most likely constituents spanning 21 text elements...\n",
      "Finding the most likely constituents spanning 22 text elements...\n",
      "Finding the most likely constituents spanning 23 text elements...\n",
      "Finding the most likely constituents spanning 24 text elements...\n",
      "Finding the most likely constituents spanning 25 text elements...\n",
      "Finding the most likely constituents spanning 26 text elements...\n",
      "Finding the most likely constituents spanning 27 text elements...\n",
      "Finding the most likely constituents spanning 28 text elements...\n",
      "Finding the most likely constituents spanning 29 text elements...\n",
      "Finding the most likely constituents spanning 30 text elements...\n",
      "Finding the most likely constituents spanning 31 text elements...\n",
      "Finding the most likely constituents spanning 32 text elements...\n",
      "Finding the most likely constituents spanning 33 text elements...\n",
      "Finding the most likely constituents spanning 34 text elements...\n",
      "Finding the most likely constituents spanning 35 text elements...\n",
      "Finding the most likely constituents spanning 36 text elements...\n",
      "Finding the most likely constituents spanning 37 text elements...\n",
      "Finding the most likely constituents spanning 38 text elements...\n",
      "   Insert: |======================================| S -> ADVP NP1 CONJ NP2 CONJ NP3 PNP CONJ NP4 [0.5] 0.0000000000 \n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"qkJhfhB9ZqTTlJFN2I6GTK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "num_parses = len(parses)\n",
    "print(\"Number of parses: \", num_parses)\n",
    "\n",
    "for p in parses:\n",
    "    all_parses[p.freeze()] = 1\n",
    "\n",
    "print(\"---------------------------------------------------------\")\n",
    "print_stats(all_parses, num_parses)\n",
    "index = index + 1"
   ],
   "execution_count":38,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Number of parses:  1\n",
      "---------------------------------------------------------\n",
      "\n",
      "Time (secs)   # Parses   Average P(parse)\n",
      "-----------------------------------------\n",
      "     1.8852          1   0.00000000000000\n",
      "------------------------------------------\n",
      "        n\/a          1   0.00000000000000\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Printing parses\n",
      "------------------------------------\n",
      "(S\n",
      "  (ADVP\n",
      "    (ADV sometimes)\n",
      "    (ADP while)\n",
      "    (V playing)\n",
      "    (DET a)\n",
      "    (N game))\n",
      "  (NP1\n",
      "    (PN you)\n",
      "    (V can)\n",
      "    (V answer)\n",
      "    (DET a)\n",
      "    (N question)\n",
      "    (ADV correctly))\n",
      "  (CONJ but)\n",
      "  (NP2\n",
      "    (N alexa)\n",
      "    (VNP (V says) (PN you))\n",
      "    (VNP (V got) (PN it))\n",
      "    (ADJ wrong))\n",
      "  (CONJ and)\n",
      "  (NP3 (V answers) (DET the) (ADJ same) (ADP as) (PN you))\n",
      "  (PNP\n",
      "    (PN i)\n",
      "    (VP (V like) (V being) (V able))\n",
      "    (PRT to)\n",
      "    (V turn)\n",
      "    (N lights)\n",
      "    (ADP on))\n",
      "  (CONJ and)\n",
      "  (NP4\n",
      "    (ADP off)\n",
      "    (ADP while)\n",
      "    (ADV away)\n",
      "    (ADP from)\n",
      "    (N home))) [4.786055859375003e-31]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"e8CzBqwloRovgCwjgre2TC",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Taking Row 4 from dataset"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"87Jgij8lWksN6QEjTjkmit",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "sent, grammar = sentences[index]\n",
    "sent"
   ],
   "execution_count":39,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "'i have had a lot of fun with this thing my 4 yr old learns about dinosaurs i control the lights and play games like categories has nice sound when playing music as well'"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"ClTIz7r6dhYLR9fINxMHes",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "# Tokenize the sentence.\n",
    "tokens = sent.split()\n",
    "\n",
    "parser = ViterbiParser(grammar)\n",
    "all_parses = {}\n",
    "\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(f\"\\nsentence: {sent}\\nparser: {parser}\\ngrammar: {grammar}\")\n",
    "print(\"---------------------------------------------------------\\n\\n\\n\")"
   ],
   "execution_count":40,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "---------------------------------------------------------\n",
      "\n",
      "sentence: i have had a lot of fun with this thing my 4 yr old learns about dinosaurs i control the lights and play games like categories has nice sound when playing music as well\n",
      "parser: <ViterbiParser for <Grammar with 78 productions>>\n",
      "grammar: Grammar with 78 productions (start state = S)\n",
      "    S -> ADVP NP1 CONJ NP2 CONJ NP3 PNP CONJ NP4 [0.5]\n",
      "    S -> S1 S2 S3 [0.5]\n",
      "    S1 -> PN V V DET N ADP N ADP DET N [1.0]\n",
      "    S2 -> NP5 NP6 CONJ NP7 [1.0]\n",
      "    S3 -> NP8 ADV NP9 [1.0]\n",
      "    NP5 -> PN NUM ADV ADJ V ADP N [1.0]\n",
      "    NP6 -> PN V DET N [1.0]\n",
      "    NP7 -> V N ADP N [1.0]\n",
      "    NP8 -> V ADJ N [1.0]\n",
      "    NP9 -> V N ADV ADV [1.0]\n",
      "    ADVP -> ADV ADP V DET N [1.0]\n",
      "    NP1 -> PN V V DET N ADV [1.0]\n",
      "    NUM -> '4' [1.0]\n",
      "    NP2 -> N VNP VNP ADJ [1.0]\n",
      "    VNP -> V PN [1.0]\n",
      "    NP3 -> V DET ADJ ADP PN [1.0]\n",
      "    VP -> V V V [1.0]\n",
      "    NP4 -> ADP ADP ADV ADP N [1.0]\n",
      "    PNP -> PN VP PRT V N ADP [1.0]\n",
      "    AD -> ADP ADP [1.0]\n",
      "    PRT -> 'to' [1.0]\n",
      "    CONJ -> 'and' [0.7]\n",
      "    CONJ -> 'but' [0.3]\n",
      "    N -> 'lot' [0.05]\n",
      "    N -> 'games' [0.1]\n",
      "    N -> 'fun' [0.05]\n",
      "    N -> 'thing' [0.1]\n",
      "    N -> 'dinosaurs' [0.05]\n",
      "    N -> 'music' [0.1]\n",
      "    N -> 'categories' [0.05]\n",
      "    N -> 'sound' [0.05]\n",
      "    N -> 'game' [0.05]\n",
      "    N -> 'question' [0.1]\n",
      "    N -> 'alexa' [0.1]\n",
      "    N -> 'lights' [0.1]\n",
      "    N -> 'home' [0.1]\n",
      "    PN -> 'my' [0.1]\n",
      "    PN -> 'you' [0.7]\n",
      "    PN -> 'it' [0.1]\n",
      "    PN -> 'i' [0.1]\n",
      "    ADJ -> 'old' [0.25]\n",
      "    ADJ -> 'nice' [0.25]\n",
      "    ADJ -> 'wrong' [0.25]\n",
      "    ADJ -> 'same' [0.25]\n",
      "    ADV -> 'well' [0.1]\n",
      "    ADV -> 'when' [0.2]\n",
      "    ADV -> 'yr' [0.1]\n",
      "    ADV -> 'sometimes' [0.2]\n",
      "    ADV -> 'correctly' [0.2]\n",
      "    ADV -> 'away' [0.2]\n",
      "    ADP -> 'of' [0.1]\n",
      "    ADP -> 'with' [0.1]\n",
      "    ADP -> 'about' [0.1]\n",
      "    ADP -> 'while' [0.3]\n",
      "    ADP -> 'as' [0.1]\n",
      "    ADP -> 'on' [0.1]\n",
      "    ADP -> 'from' [0.1]\n",
      "    ADP -> 'off' [0.1]\n",
      "    V -> 'control' [0.025]\n",
      "    V -> 'have' [0.05]\n",
      "    V -> 'had' [0.05]\n",
      "    V -> 'learns' [0.05]\n",
      "    V -> 'play' [0.125]\n",
      "    V -> 'has' [0.05]\n",
      "    V -> 'playing' [0.1]\n",
      "    V -> 'can' [0.05]\n",
      "    V -> 'answer' [0.1]\n",
      "    V -> 'says' [0.1]\n",
      "    V -> 'got' [0.05]\n",
      "    V -> 'answers' [0.05]\n",
      "    V -> 'like' [0.05]\n",
      "    V -> 'being' [0.05]\n",
      "    V -> 'able' [0.05]\n",
      "    V -> 'turn' [0.05]\n",
      "    DET -> 'a' [0.6]\n",
      "    DET -> 'the' [0.3]\n",
      "    DET -> 'this' [0.1]\n",
      "    DET ->  [0]\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"jIqGeM19P4lY4CP2uGTqSu",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import time\n",
    "\n",
    "parser.trace(3)\n",
    "t = time.time()\n",
    "parses = parser.parse_all(tokens)\n",
    "time = time.time() - t\n",
    "average = (\n",
    "    reduce(lambda a, b: a + b.prob(), parses, 0) \/ len(parses) if parses else 0\n",
    ")"
   ],
   "execution_count":41,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Inserting tokens into the most likely constituents table...\n",
      "   Insert: |=.................................| i\n",
      "   Insert: |.=................................| have\n",
      "   Insert: |..=...............................| had\n",
      "   Insert: |...=..............................| a\n",
      "   Insert: |....=.............................| lot\n",
      "   Insert: |.....=............................| of\n",
      "   Insert: |......=...........................| fun\n",
      "   Insert: |.......=..........................| with\n",
      "   Insert: |........=.........................| this\n",
      "   Insert: |.........=........................| thing\n",
      "   Insert: |..........=.......................| my\n",
      "   Insert: |...........=......................| 4\n",
      "   Insert: |............=.....................| yr\n",
      "   Insert: |.............=....................| old\n",
      "   Insert: |..............=...................| learns\n",
      "   Insert: |...............=..................| about\n",
      "   Insert: |................=.................| dinosaurs\n",
      "   Insert: |.................=................| i\n",
      "   Insert: |..................=...............| control\n",
      "   Insert: |...................=..............| the\n",
      "   Insert: |....................=.............| lights\n",
      "   Insert: |.....................=............| and\n",
      "   Insert: |......................=...........| play\n",
      "   Insert: |.......................=..........| games\n",
      "   Insert: |........................=.........| like\n",
      "   Insert: |.........................=........| categories\n",
      "   Insert: |..........................=.......| has\n",
      "   Insert: |...........................=......| nice\n",
      "   Insert: |............................=.....| sound\n",
      "   Insert: |.............................=....| when\n",
      "   Insert: |..............................=...| playing\n",
      "   Insert: |...............................=..| music\n",
      "   Insert: |................................=.| as\n",
      "   Insert: |.................................=| well\n",
      "Finding the most likely constituents spanning 1 text elements...\n",
      "   Insert: |=.................................| PN -> 'i' [0.1] 0.1000000000 \n",
      "   Insert: |.=................................| V -> 'have' [0.05] 0.0500000000 \n",
      "   Insert: |..=...............................| V -> 'had' [0.05] 0.0500000000 \n",
      "   Insert: |...=..............................| DET -> 'a' [0.6] 0.6000000000 \n",
      "   Insert: |....=.............................| N -> 'lot' [0.05] 0.0500000000 \n",
      "   Insert: |.....=............................| ADP -> 'of' [0.1] 0.1000000000 \n",
      "   Insert: |......=...........................| N -> 'fun' [0.05] 0.0500000000 \n",
      "   Insert: |.......=..........................| ADP -> 'with' [0.1] 0.1000000000 \n",
      "   Insert: |........=.........................| DET -> 'this' [0.1] 0.1000000000 \n",
      "   Insert: |.........=........................| N -> 'thing' [0.1] 0.1000000000 \n",
      "   Insert: |..........=.......................| PN -> 'my' [0.1] 0.1000000000 \n",
      "   Insert: |...........=......................| NUM -> '4' [1.0] 1.0000000000 \n",
      "   Insert: |............=.....................| ADV -> 'yr' [0.1] 0.1000000000 \n",
      "   Insert: |.............=....................| ADJ -> 'old' [0.25] 0.2500000000 \n",
      "   Insert: |..............=...................| V -> 'learns' [0.05] 0.0500000000 \n",
      "   Insert: |...............=..................| ADP -> 'about' [0.1] 0.1000000000 \n",
      "   Insert: |................=.................| N -> 'dinosaurs' [0.05] 0.0500000000 \n",
      "   Insert: |.................=................| PN -> 'i' [0.1] 0.1000000000 \n",
      "   Insert: |..................=...............| V -> 'control' [0.025] 0.0250000000 \n",
      "   Insert: |...................=..............| DET -> 'the' [0.3] 0.3000000000 \n",
      "   Insert: |....................=.............| N -> 'lights' [0.1] 0.1000000000 \n",
      "   Insert: |.....................=............| CONJ -> 'and' [0.7] 0.7000000000 \n",
      "   Insert: |......................=...........| V -> 'play' [0.125] 0.1250000000 \n",
      "   Insert: |.......................=..........| N -> 'games' [0.1] 0.1000000000 \n",
      "   Insert: |........................=.........| V -> 'like' [0.05] 0.0500000000 \n",
      "   Insert: |.........................=........| N -> 'categories' [0.05] 0.0500000000 \n",
      "   Insert: |..........................=.......| V -> 'has' [0.05] 0.0500000000 \n",
      "   Insert: |...........................=......| ADJ -> 'nice' [0.25] 0.2500000000 \n",
      "   Insert: |............................=.....| N -> 'sound' [0.05] 0.0500000000 \n",
      "   Insert: |.............................=....| ADV -> 'when' [0.2] 0.2000000000 \n",
      "   Insert: |..............................=...| V -> 'playing' [0.1] 0.1000000000 \n",
      "   Insert: |...............................=..| N -> 'music' [0.1] 0.1000000000 \n",
      "   Insert: |................................=.| ADP -> 'as' [0.1] 0.1000000000 \n",
      "   Insert: |.................................=| ADV -> 'well' [0.1] 0.1000000000 \n",
      "Finding the most likely constituents spanning 2 text elements...\n",
      "Finding the most likely constituents spanning 3 text elements...\n",
      "   Insert: |..........................===.....| NP8 -> V ADJ N [1.0] 0.0006250000 \n",
      "Finding the most likely constituents spanning 4 text elements...\n",
      "   Insert: |.................====.............| NP6 -> PN V DET N [1.0] 0.0000750000 \n",
      "Finding the most likely constituents spanning 5 text elements...\n",
      "Finding the most likely constituents spanning 6 text elements...\n",
      "Finding the most likely constituents spanning 7 text elements...\n",
      "   Insert: |..........=======.................| NP5 -> PN NUM ADV ADJ V ADP N [1.0] 0.0000006250 \n",
      "Finding the most likely constituents spanning 8 text elements...\n",
      "Finding the most likely constituents spanning 9 text elements...\n",
      "Finding the most likely constituents spanning 10 text elements...\n",
      "   Insert: |==========........................| S1 -> PN V V DET N ADP N ADP DET N [1.0] 0.0000000000 \n",
      "Finding the most likely constituents spanning 11 text elements...\n",
      "Finding the most likely constituents spanning 12 text elements...\n",
      "Finding the most likely constituents spanning 13 text elements...\n",
      "Finding the most likely constituents spanning 14 text elements...\n",
      "Finding the most likely constituents spanning 15 text elements...\n",
      "Finding the most likely constituents spanning 16 text elements...\n",
      "Finding the most likely constituents spanning 17 text elements...\n",
      "Finding the most likely constituents spanning 18 text elements...\n",
      "Finding the most likely constituents spanning 19 text elements...\n",
      "Finding the most likely constituents spanning 20 text elements...\n",
      "Finding the most likely constituents spanning 21 text elements...\n",
      "Finding the most likely constituents spanning 22 text elements...\n",
      "Finding the most likely constituents spanning 23 text elements...\n",
      "Finding the most likely constituents spanning 24 text elements...\n",
      "Finding the most likely constituents spanning 25 text elements...\n",
      "Finding the most likely constituents spanning 26 text elements...\n",
      "Finding the most likely constituents spanning 27 text elements...\n",
      "Finding the most likely constituents spanning 28 text elements...\n",
      "Finding the most likely constituents spanning 29 text elements...\n",
      "Finding the most likely constituents spanning 30 text elements...\n",
      "Finding the most likely constituents spanning 31 text elements...\n",
      "Finding the most likely constituents spanning 32 text elements...\n",
      "Finding the most likely constituents spanning 33 text elements...\n",
      "Finding the most likely constituents spanning 34 text elements...\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"0cHdOACpBUhiWIdk5SfyIX",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "num_parses = len(parses)\n",
    "print(\"Number of parses: \", num_parses)\n",
    "\n",
    "for p in parses:\n",
    "    all_parses[p.freeze()] = 1\n",
    "\n",
    "print(\"---------------------------------------------------------\")\n",
    "print_stats(all_parses, num_parses)"
   ],
   "execution_count":42,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Number of parses:  0\n",
      "---------------------------------------------------------\n",
      "\n",
      "Time (secs)   # Parses   Average P(parse)\n",
      "-----------------------------------------\n",
      "     1.2253          0   0.00000000000000\n",
      "------------------------------------------\n",
      "        n\/a          0   0.00000000000000\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"iTpX0YAIu2Xu7vIWrNbmFc",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"QOv7VXH2QVVqqwsB5XunY8",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ],
   "report_row_ids":[
    
   ],
   "version":2
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}